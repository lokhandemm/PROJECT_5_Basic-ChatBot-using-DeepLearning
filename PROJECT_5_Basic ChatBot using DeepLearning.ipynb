{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "64e8ba99-e304-4f72-937f-14cd4facc709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "19435938-7d32-4a41-9c83-65a3af6d9280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "051c0cb2-811d-4022-88fe-5cd422cf3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-preprocessing in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras-preprocessing) (1.26.4)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras-preprocessing) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cc673a84-94af-4dd8-97a4-ef50a2859813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b8dc10d7-169f-453f-bf56-7eeaa58f49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26c15ac5-7746-41bc-a7e4-28e1c44dd9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.18.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0a0fb329-87fb-483a-aa15-2d32cb6db221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-preprocessing in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras-preprocessing) (1.26.4)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras-preprocessing) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e8927325-a56f-40c9-b8af-d82829103c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model \n",
    "from tensorflow.keras.layers import Embedding \n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate, LSTM \n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "from keras_preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3ab92e35-3edc-49a6-a684-bd09fe661439",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\hp\\\\Downloads\\\\train_qa.txt', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('C:\\\\Users\\\\hp\\\\Downloads\\\\test_qa.txt', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "46b37c9a-5595-4be1-b48b-82bcd35f0f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ca44f523-df78-4b2f-9c4d-3e09bc1f4214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c0cd5380-0d03-462a-b8cd-9c772e958a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1f188d10-b13f-4274-af5d-fc2f56c07910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f86e834f-f4ae-474a-a743-63856bea77b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
       " 'no')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6875af3f-0d4b-4d17-8366-b6e3305e72d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is', 'Sandra', 'in', 'the', 'hallway', '?']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bc0e3de8-0a15-40a2-8f6b-183e603d0e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary',\n",
       " 'moved',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bathroom',\n",
       " '.',\n",
       " 'Sandra',\n",
       " 'journeyed',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bedroom',\n",
       " '.']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5cd2f7d8-e37e-4342-9863-feabfb7dfc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f133bf8d-5e05-4371-a736-9a7c1cae3907",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = test_data + train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1d4ab510-68fd-4b71-8e48-a99493784973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "eb63ca83-ee19-4758-a118-55c0cef0f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for story, question, answer in all_data:\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b115e1c8-4e43-4809-baf2-37d21f5f76b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c1ea7ce1-5898-402c-921a-3141e4c9d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "53d48e55-867d-4885-94fc-2cdc2de873a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len=len(vocab)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7975e86f-b5c9-45bf-b449-32426b81afff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "70a8138b-ca9f-4836-99a6-159b03d6ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of all the stories\n",
    "all_story_len = [len(data[0]) for data in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "4cc997ba-71d5-4027-81d0-790d3ce0b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximum of the stories\n",
    "max_story_len = max(all_story_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "7e26bba8-f63b-4b38-a079-0ff1d9ee02ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max_story_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1b069d00-7940-46c1-a584-3c775da02103",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len = max([len(data[1]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1ac302d8-fa7a-44c1-8209-020fe373e014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "09887930-747d-4614-8824-3d7169f2a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2e20f7f9-a3df-4bb2-801e-80d8cf165dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moved': 1,\n",
       " '?': 2,\n",
       " 'garden': 3,\n",
       " 'is': 4,\n",
       " 'got': 5,\n",
       " 'dropped': 6,\n",
       " 'to': 7,\n",
       " 'the': 8,\n",
       " 'sandra': 9,\n",
       " 'back': 10,\n",
       " 'kitchen': 11,\n",
       " 'bedroom': 12,\n",
       " 'went': 13,\n",
       " 'there': 14,\n",
       " 'john': 15,\n",
       " 'up': 16,\n",
       " 'football': 17,\n",
       " 'left': 18,\n",
       " 'travelled': 19,\n",
       " 'took': 20,\n",
       " 'hallway': 21,\n",
       " 'mary': 22,\n",
       " 'picked': 23,\n",
       " 'journeyed': 24,\n",
       " 'apple': 25,\n",
       " 'in': 26,\n",
       " 'bathroom': 27,\n",
       " 'daniel': 28,\n",
       " 'discarded': 29,\n",
       " 'yes': 30,\n",
       " 'grabbed': 31,\n",
       " 'office': 32,\n",
       " '.': 33,\n",
       " 'put': 34,\n",
       " 'milk': 35,\n",
       " 'no': 36,\n",
       " 'down': 37}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "f0032c70-a84f-468f-bd65-018d738c4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d258f105-3d6e-4ae5-bfb5-9d817c1678be",
   "metadata": {},
   "source": [
    "for story, question, answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)\n",
    "    train_answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "0288b51b-ea9d-4eb0-91d6-610f5d8611d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_seq= tokenizer.texts_to_sequences(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "728deb47-3520-41de-b627-eff2e5eab3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "adcba955-1ef5-4763-82c7-0548149e704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.'], ['Is', 'Sandra', 'in', 'the', 'hallway', '?'], 'no'), (['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.'], ['Is', 'Daniel', 'in', 'the', 'bathroom', '?'], 'no')]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:2])  # Check first two entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b9ad3acb-e691-4e0f-9796-4b712a937752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_data: 10000\n",
      "First 2 items of train_data: [(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.'], ['Is', 'Sandra', 'in', 'the', 'hallway', '?'], 'no'), (['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.'], ['Is', 'Daniel', 'in', 'the', 'bathroom', '?'], 'no')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train_data:\", len(train_data))\n",
    "print(\"First 2 items of train_data:\", train_data[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4ecd4330-f233-4516-ab28-16cf54b66b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing item 1:\n",
      "Story: ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.']\n",
      "Question: ['Is', 'Sandra', 'in', 'the', 'hallway', '?']\n",
      "Answer: no\n",
      "Processing item 2:\n",
      "Story: ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.']\n",
      "Question: ['Is', 'Daniel', 'in', 'the', 'bathroom', '?']\n",
      "Answer: no\n",
      "Processing item 3:\n",
      "Story: ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.', 'Sandra', 'went', 'to', 'the', 'kitchen', '.', 'Daniel', 'went', 'back', 'to', 'the', 'bathroom', '.']\n",
      "Question: ['Is', 'Daniel', 'in', 'the', 'office', '?']\n",
      "Answer: no\n",
      "Processing item 4:\n",
      "Story: ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.', 'Sandra', 'went', 'to', 'the', 'kitchen', '.', 'Daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'Daniel', 'picked', 'up', 'the', 'football', 'there', '.', 'Daniel', 'went', 'to', 'the', 'bedroom', '.']\n",
      "Question: ['Is', 'Daniel', 'in', 'the', 'bedroom', '?']\n",
      "Answer: yes\n",
      "Processing item 5:\n",
      "Story: ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.', 'Sandra', 'went', 'to', 'the', 'kitchen', '.', 'Daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'Daniel', 'picked', 'up', 'the', 'football', 'there', '.', 'Daniel', 'went', 'to', 'the', 'bedroom', '.', 'John', 'travelled', 'to', 'the', 'office', '.', 'Sandra', 'went', 'to', 'the', 'garden', '.']\n",
      "Question: ['Is', 'Daniel', 'in', 'the', 'bedroom', '?']\n",
      "Answer: yes\n",
      "Final train_story_text length: 10000\n",
      "First 2 stories: [['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.'], ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.']]\n"
     ]
    }
   ],
   "source": [
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for i, (story, question, answer) in enumerate(train_data):\n",
    "    if i < 5:  # Print only the first few iterations\n",
    "        print(f\"Processing item {i+1}:\")\n",
    "        print(\"Story:\", story)\n",
    "        print(\"Question:\", question)\n",
    "        print(\"Answer:\", answer)\n",
    "        \n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)\n",
    "    train_answers.append(answer)\n",
    "\n",
    "print(\"Final train_story_text length:\", len(train_story_text))  # Should be 10,000\n",
    "print(\"First 2 stories:\", train_story_text[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ed248e19-18e4-4e1a-a181-c064dd1f2197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before potential overwrite: [['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.'], ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before potential overwrite:\", train_story_text[:2])\n",
    "# Look for something like: train_story_text = []  # This would reset it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ac0df095-7300-4588-ac93-ea70641859f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.',\n",
       "  'Mary',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.',\n",
       "  'Daniel',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hallway',\n",
       "  '.']]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train_story_text is a list of lists of words\n",
    "train_story_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e5aae6d8-77ea-40ad-9c96-2c529c100c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(train_story_text)\n",
    "\n",
    "# Now convert texts to sequences\n",
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)\n",
    "\n",
    "print(len(train_story_seq))  # Should print the correct length, like 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "266fd2c9-50f9-4df8-942c-2c3f964a7e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.'], ['Is', 'Sandra', 'in', 'the', 'hallway', '?'], 'no'), (['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.'], ['Is', 'Daniel', 'in', 'the', 'bathroom', '?'], 'no'), (['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.', 'Sandra', 'went', 'to', 'the', 'kitchen', '.', 'Daniel', 'went', 'back', 'to', 'the', 'bathroom', '.'], ['Is', 'Daniel', 'in', 'the', 'office', '?'], 'no'), (['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.', 'Sandra', 'went', 'to', 'the', 'kitchen', '.', 'Daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'Daniel', 'picked', 'up', 'the', 'football', 'there', '.', 'Daniel', 'went', 'to', 'the', 'bedroom', '.'], ['Is', 'Daniel', 'in', 'the', 'bedroom', '?'], 'yes'), (['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.', 'Sandra', 'went', 'to', 'the', 'kitchen', '.', 'Daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'Daniel', 'picked', 'up', 'the', 'football', 'there', '.', 'Daniel', 'went', 'to', 'the', 'bedroom', '.', 'John', 'travelled', 'to', 'the', 'office', '.', 'Sandra', 'went', 'to', 'the', 'garden', '.'], ['Is', 'Daniel', 'in', 'the', 'bedroom', '?'], 'yes')]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))  # Should be around 1000\n",
    "print(train_data[:5])   # Print first 5 items in train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "57b5c5ee-ba4b-4973-a288-d36ca138704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stories in train_data: 10000\n",
      "First 2 stories in train_data: [(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.'], ['Is', 'Sandra', 'in', 'the', 'hallway', '?'], 'no'), (['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.'], ['Is', 'Daniel', 'in', 'the', 'bathroom', '?'], 'no')]\n"
     ]
    }
   ],
   "source": [
    "# Check if train_data is properly populated\n",
    "print(f\"Number of stories in train_data: {len(train_data)}\")\n",
    "print(f\"First 2 stories in train_data: {train_data[:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "38b425cd-0b27-4041-985a-b68ec0d2ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in train_data: 10000\n",
      "Story 0: ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.']\n",
      "Question 0: ['Is', 'Sandra', 'in', 'the', 'hallway', '?']\n",
      "Answer 0: no\n",
      "Story 1: ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.']\n",
      "Question 1: ['Is', 'Daniel', 'in', 'the', 'bathroom', '?']\n",
      "Answer 1: no\n",
      "Length of train_story_text: 10000\n",
      "First 2 stories in train_story_text: [['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.'], ['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lists\n",
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "# Check if train_data is being accessed correctly\n",
    "print(f\"Total entries in train_data: {len(train_data)}\")\n",
    "\n",
    "for idx, (story, question, answer) in enumerate(train_data):\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)\n",
    "    train_answers.append(answer)\n",
    "\n",
    "    # Check first few entries to ensure appending works\n",
    "    if idx < 2:\n",
    "        print(f\"Story {idx}: {story}\")\n",
    "        print(f\"Question {idx}: {question}\")\n",
    "        print(f\"Answer {idx}: {answer}\")\n",
    "\n",
    "# After the loop, check the length\n",
    "print(f\"Length of train_story_text: {len(train_story_text)}\")\n",
    "print(f\"First 2 stories in train_story_text: {train_story_text[:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "f10e3bba-9fb0-4d39-bc99-a48c70d88c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_story_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "5fa27275-8f4a-4bd2-866b-9c4b6467bf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "e2d1be80-047b-4b39-a351-d4cd64f7e860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 12, 3, 1, 19, 2, 5, 10, 3, 1, 14, 2],\n",
       " [6,\n",
       "  12,\n",
       "  3,\n",
       "  1,\n",
       "  19,\n",
       "  2,\n",
       "  5,\n",
       "  10,\n",
       "  3,\n",
       "  1,\n",
       "  14,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  13,\n",
       "  3,\n",
       "  1,\n",
       "  14,\n",
       "  2,\n",
       "  8,\n",
       "  4,\n",
       "  13,\n",
       "  3,\n",
       "  1,\n",
       "  16,\n",
       "  2]]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_story_seq[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b0a738f8-5394-46a2-bf75-aa07f0a71c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own list of list of word indicies with padding.\n",
    "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len, max_question_len=max_question_len):\n",
    "    # Stories = X\n",
    "    X = []\n",
    "    \n",
    "    # Questions = Xq\n",
    "    Xq = []\n",
    "    \n",
    "    # Y Correct Answer ['yes', 'no']\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        \n",
    "        # for each story\n",
    "        # [23, 14, 15]\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        y = np.zeros(len(word_index)+1)\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        X.append(x)   # X holds list of lists of word indices for stories.\n",
    "        Xq.append(xq) # Xq holds list of lists for word indices for questions.\n",
    "        Y.append(y) # Y holds lists of lists of (38) biniary numbers, only 1 of them is 1.\n",
    "        \n",
    "    return (pad_sequences(X, maxlen=max_story_len), pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7aa21d12-4f36-40b0-8500-2c262f0b5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own list of list of word indicies with padding.\n",
    "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len, max_question_len=max_question_len):\n",
    "    # Stories = X\n",
    "    X = []\n",
    "    \n",
    "    # Questions = Xq\n",
    "    Xq = []\n",
    "    \n",
    "    # Y Correct Answer ['yes', 'no']\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        \n",
    "        # for each story\n",
    "        # [23, 14, 15]\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        y = np.zeros(len(word_index)+1)\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        X.append(x)   # X holds list of lists of word indices for stories.\n",
    "        Xq.append(xq) # Xq holds list of lists for word indices for questions.\n",
    "        Y.append(y) # Y holds lists of lists of (38) biniary numbers, only 1 of them is 1.\n",
    "        \n",
    "    return (pad_sequences(X, maxlen=max_story_len), pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "68c3b23d-24e7-44b3-b888-b6c2764f5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data,word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "f65e8e82-e046-4b67-9b71-6ff4bc2e9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data,word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c09507b9-d6a7-468e-8e1a-13eb08c845bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 4, 0, 0],\n",
       "       [0, 0, 0, ..., 4, 0, 0],\n",
       "       [0, 0, 0, ..., 4, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 4, 0, 0],\n",
       "       [0, 0, 0, ..., 4, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "df03a6f4-a43a-45c2-9b34-8c278b285866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  4,  0,  0],\n",
       "       [ 0,  0,  0, ...,  4,  0,  0],\n",
       "       [ 0,  0,  0, ...,  4,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  4,  0,  0],\n",
       "       [ 0,  0,  0, ..., 11,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "57264c41-d549-425d-a408-a5a6c9c78f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "414af5ea-8974-4e03-b10d-974e73752808",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'yes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[328], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'yes'"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "c7027eb1-f9a5-43c9-8c04-b27d1984ceae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'no'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[330], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'no'"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "5f46369d-f654-4ef7-8f85-d30aad000395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 497 of the questions have answer 'yes', 503 of the questions have answer 'no'.\n",
    "sum(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe84399-4cd9-4a48-a5b1-65a9f0f48bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
